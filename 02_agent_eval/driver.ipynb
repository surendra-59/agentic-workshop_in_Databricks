{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "769cb73b-e013-4b33-bcfc-fc23dfb3a211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7041ced0-d931-4636-8118-d3cf8207d51d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Hands-On Lab: Building Agent Systems with Databricks\n",
    "\n",
    "## Part 2 - Agent Evaluation\n",
    "Now that we've created an agent, how do we evaluate its performance?\n",
    "For the second part, we're going to create a product support agent so we can focus on evaluation.\n",
    "This agent will use a RAG approach to help answer questions about products using the product documentation.\n",
    "\n",
    "### 2.1 Define our new Agent and retriever tool\n",
    "- [**agent.py**]($./agent.py): An example Agent has been configured - first we'll explore this file and understand the building blocks\n",
    "- **Vector Search**: We've created a Vector Search endpoint that can be queried to find related documentation about a specific product.\n",
    "- **Create Retriever Function**: Define some properties about our retriever and package it so it can be called by our LLM.\n",
    "\n",
    "### 2.2 Create Evaluation Dataset\n",
    "- We've provided an example evaluation dataset - though you can also generate this [synthetically](https://www.databricks.com/blog/streamline-ai-agent-evaluation-with-new-synthetic-data-capabilities).\n",
    "\n",
    "### 2.3 Run MLflow.genai.evaluate() \n",
    "- MLflow will take your evaluation dataset and test your agent's responses against it\n",
    "- LLM Judges will score the outputs and collect everything in a nice UI for review\n",
    "\n",
    "### 2.4 Make Needed Improvements and re-run Evaluations\n",
    "- Take feedback from our evaluation run and change the prompt\n",
    "- Run evals again and see the improvement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a19e9d3-dd3a-4fbb-af93-2f033fb9c665",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Libraries"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyter-server 1.23.4 requires anyio<4,>=3.1.0, but you have anyio 4.12.1 which is incompatible.\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -qqqq backoff databricks-openai uv databricks-agents mlflow-skinny[databricks] unitycatalog-langchain[databricks] databricks-langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbc7ceea-ac9e-4b85-965c-ace76000fc2d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8509e995-18d8-4030-817d-ec50c07464e7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Warning Suppression"
    }
   },
   "outputs": [],
   "source": [
    "# --- Lab hygiene: suppress known non-actionable warnings ---\n",
    "import warnings, logging\n",
    "\n",
    "# Pydantic v2 serializer warnings (safe to ignore for this lab)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\"^Pydantic serializer warnings:\",\n",
    "    category=UserWarning,\n",
    "    module=r\"pydantic\\..*\",\n",
    ")\n",
    "\n",
    "# MLflow tracing warnings (otel not fully enabled in this runtime)\n",
    "logging.getLogger(\"mlflow.tracing\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"mlflow.tracing.fluent\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf21af87-a1ce-469e-ba7d-fde49bd8e998",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ensure Latest Agent Definition"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e4981c6-f8a3-426d-ac44-9a2118dbe1e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Quick test to see if Agent works"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-8e27ae27-66fb-4131-b8a9-ca619bf247b3/lib/python3.10/site-packages/databricks/connect/session.py:451: UserWarning: Ignoring the default notebook Spark session and creating a new Spark Connect session. To use the default notebook Spark session, use DatabricksSession.builder.getOrCreate() with no additional parameters.\n  warnings.warn(new_notebook_session_msg)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ResponsesAgentResponse(tool_choice=None, truncation=None, id=None, created_at=None, error=None, incomplete_details=None, instructions=None, metadata=None, model=None, object='response', output=[OutputItem(type='reasoning', summary=[{'type': 'summary_text', 'text': 'The user asks \"Hello, what do you do?\" They want to know what the assistant does. As a customer success specialist for Databricks lab. We should respond explaining role. No need to call any tool.'}], id='chatcmpl_b5de0041-b105-4d0f-bfca-c66047ea4c42'), OutputItem(type='message', id='chatcmpl_b5de0041-b105-4d0f-bfca-c66047ea4c42', content=[{'text': 'Hi there! I’m a Customer Success Specialist for the Databricks Labs program. My role is to help you get the most out of the Databricks platform and any lab‑related resources we provide. \\n\\nHere’s a quick rundown of what I can do for you:\\n\\n- **Answer product‑related questions** about Spark, Delta Lake, MLflow, Unity Catalog, and other Databricks components.  \\n- **Guide you through lab exercises**, troubleshooting steps, and best‑practice recommendations.  \\n- **Help with account‑related tasks** such as checking order history, returns, or understanding our return policy.  \\n- **Provide documentation links and examples** so you can quickly find the information you need.  \\n- **Escalate issues** to the appropriate technical or support teams when deeper investigation is required.  \\n\\nIf you have a specific question or need assistance with something in particular—whether it’s a lab setup, a coding problem, or a policy inquiry—just let me know and I’ll be happy to help!', 'type': 'output_text'}], role='assistant')], parallel_tool_calls=None, temperature=None, tools=None, top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, status=None, text=None, usage=None, user=None, custom_outputs=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "{\"trace_id\": \"tr-848126690fd65db04f2d619119dd9d64\", \"sql_warehouse_id\": null}",
      "text/plain": [
       "Trace(trace_id=tr-848126690fd65db04f2d619119dd9d64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from agent import AGENT\n",
    "\n",
    "AGENT.predict({\"input\": [{\"role\": \"user\", \"content\": \"Hello, what do you do?\"}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7577568c-db7a-4dbf-9e23-10a809e53a23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Log the `agent` as an MLflow model\n",
    "Log the agent as code from the [agent]($./agent) notebook. See [MLflow - Models from Code](https://mlflow.org/docs/latest/models.html#models-from-code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9efe0998-d34d-4293-87c2-206ea3f184ad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Log Agent in MLflow"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/13 09:29:06 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n\uD83D\uDD17 View Logged Model at: https://dbc-7aabc6fb-674f.cloud.databricks.com/ml/experiments/3390865860258818/models/m-63886b62e213408db61c7e91b4f5fa81?o=7474655838774581\n2026/02/13 09:29:07 INFO mlflow.pyfunc: Predicting on input example to validate output\n2026/02/13 09:29:07 WARNING mlflow.tracing.fluent: Failed to start span predict_stream: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n2026/02/13 09:29:07 WARNING mlflow.tracing.fluent: Failed to start span Completions: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n2026/02/13 09:29:09 WARNING mlflow.tracing.fluent: Failed to start span Completions: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n2026/02/13 09:29:25 WARNING mlflow.tracing.fluent: Failed to start span predict_stream: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n2026/02/13 09:29:25 WARNING mlflow.tracing.fluent: Failed to start span Completions: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n2026/02/13 09:29:26 WARNING mlflow.tracing.fluent: Failed to start span Completions: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n"
     ]
    }
   ],
   "source": [
    "# Determine Databricks resources to specify for automatic auth passthrough at deployment time\n",
    "import mlflow\n",
    "from agent import VECTOR_SEARCH_TOOLS, LLM_ENDPOINT_NAME\n",
    "from databricks_openai import UCFunctionToolkit, VectorSearchRetrieverTool\n",
    "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\n",
    "from unitycatalog.ai.langchain.toolkit import UnityCatalogTool\n",
    "\n",
    "\n",
    "resources = [DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)]\n",
    "for tool in VECTOR_SEARCH_TOOLS:\n",
    "    if isinstance(tool, VectorSearchRetrieverTool):\n",
    "        resources.extend(tool.resources)\n",
    "    elif isinstance(tool, UnityCatalogTool):\n",
    "        resources.append(DatabricksFunction(function_name=tool.uc_function_name))\n",
    "\n",
    "input_example = {\n",
    "    \"input\": [\n",
    "        {\"role\": \"user\", \"content\": \"What color options are available for the Aria Modern Bookshelf?\"}\n",
    "    ],\n",
    "}\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"agent\",\n",
    "        python_model=\"agent.py\",  \n",
    "        input_example=input_example,\n",
    "        resources=resources,\n",
    "        extra_pip_requirements=[\n",
    "            \"mlflow>=3.1.3\",\n",
    "            \"databricks-agents>=1.1.0\",\n",
    "            \"databricks-openai\",\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23d1decc-8b11-427a-9056-a045d9e821cf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Agent Wrapper"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3328883ed5064a2e86c6dc82fd6d0f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70566b1d24544f0af269aa67e50db03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model and create a prediction function\n",
    "logged_model_uri = f\"runs:/{logged_agent_info.run_id}/agent\"\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model_uri)\n",
    "\n",
    "def predict_wrapper(query):\n",
    "    model_input = {\n",
    "        \"input\": [\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    }\n",
    "    response = loaded_model.predict(model_input)\n",
    "    # Find the last output item of type \"message\"\n",
    "    message = next(\n",
    "        (item for item in reversed(response[\"output\"]) if item.get(\"type\") == \"message\"),\n",
    "        None\n",
    "    )\n",
    "    if message and \"content\" in message:\n",
    "        # Find the first content item of type \"output_text\"\n",
    "        content_item = next(\n",
    "            (c for c in message[\"content\"] if c.get(\"type\") == \"output_text\"),\n",
    "            None\n",
    "        )\n",
    "        if content_item:\n",
    "            return content_item[\"text\"]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3009e116-0ac8-43e8-83da-8940a740b42b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate the agent with [Agent Evaluation](https://docs.databricks.com/generative-ai/agent-evaluation/index.html)\n",
    "\n",
    "You can edit the requests or expected responses in your evaluation dataset and run evaluation as you iterate your agent, leveraging mlflow to track the computed quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b532c31-d0d6-4fe7-9f11-4db99cf23683",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Evaluation Dataset"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"request\": [\n",
    "        \"What color options are available for the Aria Modern Bookshelf?\",\n",
    "        \"How should I clean the Aurora Oak Coffee Table to avoid damaging it?\",\n",
    "        \"What sizes are available for the StormShield Pro Men's Weatherproof Jacket?\"\n",
    "    ],\n",
    "    \"expected_facts\": [\n",
    "        [\n",
    "            \"The Aria Modern Bookshelf is available in natural oak finish\",\n",
    "            \"The Aria Modern Bookshelf is available in black finish\",\n",
    "            \"The Aria Modern Bookshelf is available in white finish\"\n",
    "        ],\n",
    "        [\n",
    "            \"Use a soft, slightly damp cloth for cleaning.\",\n",
    "            \"Avoid using abrasive cleaners.\"\n",
    "        ],\n",
    "        [\n",
    "            \"The available sizes for the StormShield Pro Men's Weatherproof Jacket are Small, Medium, Large, XL, and XXL.\"\n",
    "        ]\n",
    "    ]\n",
    "}\n",
    "\n",
    "eval_dataset = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebaa5dfb-fd0f-43b7-8c17-50199fc80e25",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define our Scorers"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import RetrievalGroundedness, RelevanceToQuery, Safety, Guidelines\n",
    "import mlflow.genai\n",
    "\n",
    "eval_data = []\n",
    "for request, facts in zip(data[\"request\"], data[\"expected_facts\"]):\n",
    "    eval_data.append({\n",
    "        \"inputs\": {\n",
    "            \"query\": request  # This matches the function parameter\n",
    "        },\n",
    "        \"expected_response\": \"\\n\".join(facts)\n",
    "    })\n",
    "\n",
    "# Define custom scorers tailored to product information evaluation\n",
    "scorers = [\n",
    "    #RetrievalGroundedness(),  # Pre-defined judge that checks against retrieval results\n",
    "    RelevanceToQuery(),  # Checks if answer is relevant to the question\n",
    "    Safety(),  # Checks for harmful or inappropriate content\n",
    "    Guidelines(\n",
    "        guidelines=\"\"\"Response must be clear and direct:\n",
    "        - Answers the exact question asked\n",
    "        - Uses lists for options, steps for instructions\n",
    "        - No marketing fluff or extra background\n",
    "        - Does not tell user to contact customer support\n",
    "        - Concise but complete.\"\"\",\n",
    "        name=\"clarity_and_structure\",\n",
    "    ),\n",
    "    #Guidelines(\n",
    "    #    guidelines=\"\"\"Response must include ALL expected facts:\n",
    "    #    - Lists ALL colors/sizes if relevant (not partial lists)\n",
    "    #    - States EXACT specs if relevant (e.g., \"5 ATM\" not \"water resistant\")\n",
    "    #    - Includes ALL cleaning steps if asked\n",
    "    #    Fails if ANY fact is missing or wrong.\"\"\",\n",
    "    #    name=\"completeness_and_accuracy\",\n",
    "    #)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ae8956a-c92b-4274-840f-b803c8c20771",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run MLflow Evals"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/13 09:31:08 INFO mlflow.models.evaluation.utils.trace: Auto tracing is temporarily enabled during the model evaluation for computing some metrics and debugging. To disable tracing, call `mlflow.autolog(disable=True)`.\n2026/02/13 09:31:08 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n2026/02/13 09:31:08 WARNING mlflow.tracing.fluent: Failed to start span predict_stream: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d826ac3693047cf939b68e86a978e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"https://dbc-7aabc6fb-674f.cloud.databricks.com/ml/experiments/3390865860258818/evaluation-runs?selectedRunUuid=513866213697496ab9bb0c64b8b91ed0\" class=\"button\">\n",
       "            View evaluation results in MLflow\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Running evaluation...\")\n",
    "with mlflow.start_run():\n",
    "    results = mlflow.genai.evaluate(\n",
    "        data=eval_data,\n",
    "        predict_fn=predict_wrapper, \n",
    "        scorers=scorers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f08996a5-5392-4400-a550-ab5df516a3d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lets go back to the [agent.py]($./agent.py) file and change our prompt to better fit how we'd like it to respond and re-evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c691f82-9978-41f3-bf31-8f577f029744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register the model to Unity Catalog\n",
    "\n",
    "Update the `catalog`, `schema`, and `model_name` below to register the MLflow model to Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d88549fb-2f5c-471e-829f-b5c9bace2001",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Register Agent to UC"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'labuser13792508_1770974336.agents.product_agent'.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cd8009bfab49d6a0b9be048ddeeac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048cda65baeb4e1d88b6c03144ce1cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD17 Created version '1' of model 'labuser13792508_1770974336.agents.product_agent': https://dbc-7aabc6fb-674f.cloud.databricks.com/explore/data/models/labuser13792508_1770974336/agents/product_agent/version/1?o=7474655838774581\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import os\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Use the workspace client to retrieve information about the current user\n",
    "w = WorkspaceClient()\n",
    "user_email = w.current_user.me().display_name\n",
    "username = user_email.split(\"@\")[0]\n",
    "\n",
    "# Catalog and schema have been automatically created thanks to lab environment\n",
    "catalog_name = f\"{username}\"\n",
    "schema_name = \"agents\"\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "model_name = \"product_agent\"\n",
    "UC_MODEL_NAME = f\"{catalog_name}.{schema_name}.{model_name}\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43179dd2-55c5-4529-ba39-47b75a9526b0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Easy link to UC Reg"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<a href=\"https://dbc-7aabc6fb-674f.cloud.databricks.com/explore/data/models/labuser13792508_1770974336/agents/product_agent\" target=\"_blank\">Go to Unity Catalog to see Registered Agent</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Retrieve the Databricks host URL\n",
    "workspace_url = spark.conf.get('spark.databricks.workspaceUrl')\n",
    "\n",
    "# Create HTML link to created agent\n",
    "html_link = f'<a href=\"https://{workspace_url}/explore/data/models/{catalog_name}/{schema_name}/product_agent\" target=\"_blank\">Go to Unity Catalog to see Registered Agent</a>'\n",
    "display(HTML(html_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16e12204-b5a4-4fab-9bf9-3b59d8281e50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy the agent\n",
    "\n",
    "##### Note: This is disabled for lab users but will work on your own workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3a52299-dbc5-456e-961b-003f029668ae",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Deploy Agent"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7951ca730a164949a2e9d7640ece805e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-8e27ae27-66fb-4131-b8a9-ca619bf247b3/lib/python3.10/site-packages/databricks/agents/deployments.py:641: UserWarning: This endpoint is being deployed without a feedback model, which has been deprecated.\nFor more information, see: https://docs.databricks.com/aws/en/generative-ai/agent-framework/feedback-model\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n    Deployment of labuser13792508_1770974336.agents.product_agent version 1 initiated.  This can take up to 15 minutes and the Review App & Query Endpoint will not work until this deployment finishes.\n\n    View status: https://dbc-7aabc6fb-674f.cloud.databricks.com/ml/endpoints/agents_labuser13792508_1770974336-agents-product_agent/?o=7474655838774581\n    Review App: https://dbc-7aabc6fb-674f.cloud.databricks.com/ml/review-v2/c9b541e1f1354889ac47c0dff1eba4b2/chat?o=7474655838774581\n\nYou can refer back to the links above from the endpoint detail page at https://dbc-7aabc6fb-674f.cloud.databricks.com/ml/endpoints/agents_labuser13792508_1770974336-agents-product_agent/?o=7474655838774581.\n\nTo set up monitoring for your deployed agent, see:\nhttps://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/production-monitoring\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Deployment(model_name='labuser13792508_1770974336.agents.product_agent', model_version='1', endpoint_name='agents_labuser13792508_1770974336-agents-product_agent', served_entity_name='labuser13792508_1770974336-agents-product_agent_1', query_endpoint='https://dbc-7aabc6fb-674f.cloud.databricks.com/serving-endpoints/agents_labuser13792508_1770974336-agents-product_agent/served-models/labuser13792508_1770974336-agents-product_agent_1/invocations?o=7474655838774581', endpoint_url='https://dbc-7aabc6fb-674f.cloud.databricks.com/ml/endpoints/agents_labuser13792508_1770974336-agents-product_agent/?o=7474655838774581', review_app_url='https://dbc-7aabc6fb-674f.cloud.databricks.com/ml/review-v2/c9b541e1f1354889ac47c0dff1eba4b2/chat?o=7474655838774581')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from databricks import agents\n",
    "\n",
    "# Deploy the model to the review app and a model serving endpoint\n",
    "\n",
    "#Disabled for the lab environment but we've deployed the agent already!\n",
    "agents.deploy(UC_MODEL_NAME, uc_registered_model_info.version, tags = {\"endpointSource\": \"DI Days\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffbe0e70-607c-4f27-b266-71e64d583d5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "driver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}